{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pegasus Encoder Decoder**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training_utils import *\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from .json files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/data/PubMed/Train_ExtAbs_PUBMED.json\") as f:\n",
    "        training_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/data/PubMed/Val_ExtAbs_PUBMED.json\") as f:\n",
    "        validation_corpus = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/pegasus-x-base\" # Use pegasus-x-base-finetuned-xsum\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(tokenizer)\n",
    "print(tokenizer(text_target=[\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.num_rows = len(corpus)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_rows\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.corpus[index][0]\n",
    "        processed_example = self.preprocess_function(example)\n",
    "        return processed_example\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_function(example):\n",
    "        max_input_length = 512\n",
    "        max_target_length = 256\n",
    "        inputs = [doc for doc in example[\"article\"]]\n",
    "        model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "        print(model_inputs)\n",
    "\n",
    "        # Setup the tokenizer for targets\n",
    "        labels = tokenizer(text_target=example[\"abstract\"], max_length=max_target_length, truncation=True)\n",
    "        #print(labels)\n",
    "\n",
    "\n",
    "        return {\"article\": example[\"article\"], \n",
    "                \"abstract\": example[\"abstract\"], \n",
    "                \"input_ids\" : labels['input_ids'], \n",
    "                \"attention_mask\": model_inputs['attention_mask'],\n",
    "                \"labels\": model_inputs['labels']}\n",
    "    \n",
    "def map_function(corpus):\n",
    "    \n",
    "    tokenized_dataset = TokenizedDataset(corpus)\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_input_length = 512\n",
    "# max_target_length = 256\n",
    "\n",
    "# class TokenizedDataset(Dataset):\n",
    "#     def __init__(self,corpus):\n",
    "#         self.corpus = corpus\n",
    "#         self.features = features\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         item = preprocess_function(self.corpus[index][0])\n",
    "#         texts = self.corpus[index][0]\n",
    "        \n",
    "#         features = {texts['article'],\n",
    "#                     texts['abstract'],\n",
    "#                     item['input_ids'], \n",
    "#                     item['attention_mask'],\n",
    "#                     item['labels']}\n",
    "#         return features\n",
    "    \n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     inputs = [doc for doc in examples[\"article\"]]\n",
    "#     model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "#     # Setup the tokenizer for targets\n",
    "#     labels = tokenizer(text_target=examples[\"abstract\"], max_length=max_target_length, truncation=True)\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "#     return model_inputs\n",
    "\n",
    "# def map_function(corpus):\n",
    "    \n",
    "#     tokenized_dataset = TokenizedDataset(corpus)\n",
    "\n",
    "#     return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train = map_function(training_corpus)\n",
    "tokenized_dataset_val = map_function(validation_corpus)\n",
    "tokenized_datasets = {\"train\":tokenized_dataset_train, \"validation\":tokenized_dataset_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train[0]\n",
    "print(len(tokenized_dataset_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('ccdv/pubmed-summarization')\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = dataset[\"validation\"].map(preprocess_function, batched=True)\n",
    "token\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "metric = load(\"rouge\")\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"Pegasus-finetuned\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate = True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=512,\n",
    "    logging_steps=1,\n",
    "    label_smoothing_factor = 0.1, \n",
    "    auto_find_batch_size = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    wandb.log({'rouge1': result['rouge1'], 'rouge2': result['rouge2'], 'rougeL': result['rougeL'], 'rougeLsum': result['rougeLsum']})\n",
    "    \n",
    "\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "for param in trainer.model.model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"checkpoint-14500-finetuned_alot/checkpoint-29500\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already fine-tuned on pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/pegasus-pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Kevincp560/pegasus-arxiv-finetuned-pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pegasus\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-pubmed\")\n",
    "\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = \"Researchers at a leading university have developed a groundbreaking technology that could revolutionize renewable energy generation. The new system, known as 'SolarWave,' harnesses the power of ocean waves to generate electricity. By utilizing a network of specialized buoys equipped with advanced turbines, the technology can convert the kinetic energy from the waves into clean, sustainable power. This innovation has the potential to significantly contribute to the global efforts in combating climate change and reducing our reliance on fossil fuels. It's an exciting development that could reshape the future of renewable energy.\"\n",
    "inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"].to(device))\n",
    "output = tokenizer.batch_decode(summary_ids, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=False)[0]\n",
    "print(output)\n",
    "\n",
    "print(len(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
