{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pegasus Encoder Decoder**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training_utils import *\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "print(transformers.__version__)\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from .json files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_custom_json(file_path):\n",
    "    # Implement your custom logic to read the JSON file and extract the data\n",
    "    # For example, you can use the 'json' library or any other method you prefer\n",
    "    import json\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        # Process and extract the relevant data from the JSON file as needed\n",
    "        # Create dictionaries/lists for each feature in the dataset\n",
    "        # Return the dataset in the desired format\n",
    "        return {\n",
    "                'article':data[\"article\"],\n",
    "                'abstract':data[\"abstract\"],\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/data/PubMed/Train_ExtAbs_PUBMED.json\") as f:\n",
    "        training_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/data/PubMed/Val_ExtAbs_PUBMED.json\") as f:\n",
    "        validation_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 │   │   │   │   </span>}                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span>file_path = <span style=\"color: #808000; text-decoration-color: #808000\">\"src/data/PubMed/Train_ExtAbs_PUBMED.json\"</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>18 data = read_custom_json(file_path)                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span>dataset = Dataset.from_dict(data)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">read_custom_json</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 │   │   # Create dictionaries/lists for each feature in the dataset</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 │   │   # Return the dataset in the desired format</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> {                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>13 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">'article'</span>:data[<span style=\"color: #808000; text-decoration-color: #808000\">\"article\"</span>],                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">'abstract'</span>:data[<span style=\"color: #808000; text-decoration-color: #808000\">\"abstract\"</span>],                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 │   │   │   │   </span>}                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span>list indices must be integers or slices, not str\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m}                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0mfile_path = \u001b[33m\"\u001b[0m\u001b[33msrc/data/PubMed/Train_ExtAbs_PUBMED.json\u001b[0m\u001b[33m\"\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 data = read_custom_json(file_path)                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0mdataset = Dataset.from_dict(data)                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mread_custom_json\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Create dictionaries/lists for each feature in the dataset\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Return the dataset in the desired format\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m {                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m13 \u001b[2m│   │   │   │   \u001b[0m\u001b[33m'\u001b[0m\u001b[33marticle\u001b[0m\u001b[33m'\u001b[0m:data[\u001b[33m\"\u001b[0m\u001b[33marticle\u001b[0m\u001b[33m\"\u001b[0m],                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m'\u001b[0m\u001b[33mabstract\u001b[0m\u001b[33m'\u001b[0m:data[\u001b[33m\"\u001b[0m\u001b[33mabstract\u001b[0m\u001b[33m\"\u001b[0m],                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m}                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0mlist indices must be integers or slices, not str\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def read_custom_json(file_path):\n",
    "    # Implement your custom logic to read the JSON file and extract the data\n",
    "    # For example, you can use the 'json' library or any other method you prefer\n",
    "    import json\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        # Process and extract the relevant data from the JSON file as needed\n",
    "        # Create dictionaries/lists for each feature in the dataset\n",
    "        # Return the dataset in the desired format\n",
    "    return {'article':data[\"article\"],\n",
    "            'abstract':data[\"abstract\"],\n",
    "                } \n",
    "\n",
    "file_path = \"src/data/PubMed/Train_ExtAbs_PUBMED.json\"\n",
    "data = read_custom_json(file_path)\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/pegasus-x-base\" # Use pegasus-x-base-finetuned-xsum\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(tokenizer)\n",
    "print(tokenizer(text_target=[\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(list_of_sentences, tokenizer, max_len, tensor_type = 'np'):\n",
    "    ''' This function takes as input a list of sentences in the form of\n",
    "\n",
    "        sentences = [\n",
    "      \"This is the first sentence.\",\n",
    "      \"Here is the second sentence.\",\n",
    "      \"And another sentence.\",\n",
    "      \"A third sentence.\"\n",
    "                    ]\n",
    "    '''\n",
    "    # Tokenize the sentences as a batch\n",
    "    desired_length = max_len  # Desired length for padding\n",
    "\n",
    "    batch_encoding = tokenizer.batch_encode_plus(\n",
    "        list_of_sentences,\n",
    "        truncation=True,\n",
    "        max_length=desired_length,\n",
    "        padding='max_length',\n",
    "        return_tensors=tensor_type\n",
    "    )\n",
    "\n",
    "    input_ids = batch_encoding[\"input_ids\"]\n",
    "    attention_mask = batch_encoding[\"attention_mask\"]\n",
    "\n",
    "    return batch_encoding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self,corpus,tokenizer,device):\n",
    "        self.corpus = corpus\n",
    "        self.num_rows = len(corpus )\n",
    "        self.features = {\n",
    "                'article': '',\n",
    "                'abstract': '',\n",
    "                'input_ids': [],\n",
    "                'attention_mask': [],\n",
    "                'labels': []\n",
    "                } \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_rows\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = preprocess_function(self.corpus[index][0],tokenizer)\n",
    "        texts = self.corpus[index][0]\n",
    "        self.features['article'] = texts['article']\n",
    "        self.features['abstract'] = texts['abstract']\n",
    "        self.features['input_ids'] = item['input_ids']\n",
    "        self.features['attention_mask'] =  item['attention_mask']\n",
    "        self.features['labels'] =  item['labels']\n",
    "                    \n",
    "        return self.features\n",
    "    \n",
    "\n",
    "def preprocess_function(examples,tokenizer):\n",
    "    inputs = [doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenize_sentences(inputs, tokenizer= tokenizer, max_len=max_input_length, tensor_type='pt')#.to(device)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenize_sentences(examples[\"abstract\"], tokenizer = tokenizer, max_len=max_target_length, tensor_type='pt')#.to(device)\n",
    "\n",
    "    # Ensure labels have compatible dimensions\n",
    "    if isinstance(labels[\"input_ids\"], list):\n",
    "        labels[\"input_ids\"] = [ids[0] for ids in labels[\"input_ids\"]]  # Extract the first element\n",
    "    labels[\"input_ids\"] = [labels[\"input_ids\"]]  # Wrap the labels array in an additional list\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def map_function(corpus):\n",
    "    \n",
    "    tokenized_dataset = TokenizedDataset(corpus,tokenizer,device)\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train = map_function(training_corpus)\n",
    "tokenized_dataset_val = map_function(validation_corpus)\n",
    "tokenized_datasets = {\"train\":tokenized_dataset_train, \"validation\":tokenized_dataset_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_dataset_train[0])\n",
    "print(len(tokenized_dataset_train))\n",
    "print(tokenized_dataset_train[0].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "metric = load(\"rouge\")\n",
    "#print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"Pegasus-finetuned\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate = True,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=512,\n",
    "    logging_steps=1,\n",
    "    label_smoothing_factor = 0.1, \n",
    "    #auto_find_batch_size = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# class CustomDataCollatorForSeq2Seq:\n",
    "#     def __init__(self, tokenizer, model):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.model = model\n",
    "\n",
    "#     def __call__(self, examples):\n",
    "#         input_ids = [example['input_ids'] for example in examples]\n",
    "#         attention_mask = [example['attention_mask'] for example in examples]\n",
    "#         labels = [example['labels'] for example in examples]\n",
    "\n",
    "#         # Pad input and attention masks\n",
    "#         padded_input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "#         padded_attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
    "        \n",
    "#         # Handle dimension mismatch in labels\n",
    "#         max_label_len = max(len(label) for label in labels)\n",
    "#         padded_labels = [\n",
    "#             torch.cat((label, torch.tensor([self.tokenizer.pad_token_id] * (max_label_len - len(label)))))\n",
    "#             for label in labels\n",
    "#         ]\n",
    "        \n",
    "#         return {\n",
    "#             'input_ids': padded_input_ids,\n",
    "#             'attention_mask': padded_attention_mask,\n",
    "#             'labels': torch.stack(padded_labels),\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "#data_collator = CustomDataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    wandb.log({'rouge1': result['rouge1'], 'rouge2': result['rouge2'], 'rougeL': result['rougeL'], 'rougeLsum': result['rougeLsum']})\n",
    "    \n",
    "\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "for param in trainer.model.model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_checkpoint = \"checkpoint-14500-finetuned_alot/checkpoint-29500\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already fine-tuned on pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_checkpoint = \"google/pegasus-pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_checkpoint = \"Kevincp560/pegasus-arxiv-finetuned-pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pegasus\n",
    "# import torch\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-pubmed\")\n",
    "\n",
    "# ARTICLE_TO_SUMMARIZE = [\"Introduction: Cognitive decline is a common age-related phenomenon, and interventions to mitigate its effects are of great interest. \\\n",
    "#                         Exercise has been suggested as a potential strategy to improve cognitive function in older adults. This study aimed to investigate the effects\\\n",
    "#                          of a structured exercise program on cognitive function in elderly individuals.\\\n",
    "# Methods: A randomized controlled trial was conducted with 60 participants aged 65 and above.\\\n",
    "#                         The participants were randomly assigned to either an exercise group or a control group.\\\n",
    "#                         The exercise group underwent a 12-week exercise program consisting of aerobic exercises, strength training, \\\n",
    "#                         and flexibility exercises. The control group maintained their usual daily activities without \\\n",
    "#     any structured exercise intervention. Cognitive function was assessed using standardized neuropsychological tests at baseline and after the intervention.\\\n",
    "# Results: The results revealed significant improvements in cognitive function in the exercise group compared to the control group. \\\n",
    "#                         The exercise group demonstrated enhanced performance in various cognitive domains, including attention, memory, and executive function. \\\n",
    "#                         These improvements were statistically significant and clinically meaningful. Furthermore, the exercise group showed a significant \\\n",
    "#                         reduction in the risk of cognitive decline compared to the control group.\\\n",
    "# Conclusion: This randomized controlled trial provides evidence that a structured exercise program can have positive effects on cognitive function in elderly adults.\\\n",
    "#       Regular physical exercise, including aerobic exercises, strength training, and flexibility exercises, may serve as a valuable intervention to\\\n",
    "#                         promote cognitive health and reduce the risk of cognitive decline in the aging population.\",\n",
    "#                         \"Hi my name is jackie jack jack joock\"]\n",
    "\n",
    "# inputs = tokenize_sentences(ARTICLE_TO_SUMMARIZE, tokenizer=tokenizer, max_len=256, tensor_type = 'pt')\n",
    "\n",
    "# # Generate Summary\n",
    "# summary_ids = model.generate(inputs[\"input_ids\"].to(device)).detach()\n",
    "# output = tokenizer.batch_decode(summary_ids, skip_special_tokens=True,\n",
    "#                                 clean_up_tokenization_spaces=False)\n",
    "# print(output)\n",
    "\n",
    "# print(len(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
